# Compilation and Optimizations for Efficient Machine Learning on Embedded Systems

This paper was published the 26th of August 2022, and written by Xiaofan Zhang, Yao Chen, Cong Hao, Sitao Huang, Yuhong Li, and Deming Chen. It focuses on the fact that the development of Deep Neural Networks (DNN) have been made accessible to the public, which makes machine learning more complicated, needing more resources to run, ...\
And as the evolutions of hardware are much slower than thoses on software, it has become much more complicated to run DNN on hardware today, especially for embedded and real-time services. This paper seeks to find ways to be more efficient for DNN on embedded systems.
<br/>
<br/>

Some methods for efficient DNN design on embedded systems already exist. As DNN algorithm have millions of parameters and billions of operations, they focus on network compression to reduce network complexities and the amount of data transmitted. They can also lower computer and memory demands, for example use quantized data to represent float parameters that would take much more space.\
To reduce the redundancy of DNN structures, the method of *network pruning* has been developped. As the major benefit of network compression come from reducing the fullt-connected (FC) layer, there are significant reductions can be achieved on classic DNN structures, lke AlexNet and VGG-16. Whereas more sophisticated algorithms are required for latter DNNs with fewer FC layers.\
Other methods can be building domain-specific hardware accelerators, or even on the software side with accelerations libraries on the CPU or kernel optimization on GPU. Theses customized accelerators can be implemented on FPGAs and ASICS, but there are some challenges, such as the hardware design process, verification problems, or even the time-consuming design space exploration during the DNN development.\
Finally, researchers proposed algorithms and accelerators co-design and co-search to solve the edge AI challenge. With that, software and hardware metrics are considered simultaneously. The neural architecture search (NAS) has been successfull for DNN models as the hardware-aware NAS aims to deliver high accuracy models with hardware efficiency.
<br/>
<br/>

Machine learning requires high inference accuracy, aggressive inference speed, throughput and energy efficiency to meet the real-life demand that is increasing more and more. For that, they rely on hardware-efficient DNN.\
The extremely low bit-width neural network (ELB-NN) is proposed to enhance energy efficiency when running image classification on embedded FPGA. It has a hybrid quantization scheme, which mean that different quantization schemes are involved for the network's parameters and activations, and that can go all the way to binary.\
After doing an analysis with AlexNet, it's concluded that its strong point is the robustness of the precision of parameters, but that the precision of activation impacts classification accuracy. However, it's possible to bring back the accuracy by increasing the model complexity. On the hardware side, the ELB-NN can achieve throughput performance up to 10.3 TOPS (how many computing operations an AI accelerator can handle in one second at 100% utilization) on the accelerator, which outperforms previous designs.

Another ML algorithm is the Vectorized Quantization (VecQ), a training quantization based on a novel quantization loss measurement metric called *vector loss*. To quantisize with this vector loss, we use the square of the euclidean distance of the date before and after it to represent its loss, also called L2 distance. The orientation loss shows that the quantization takes 2 stages to minimize the 2 losses independently: the steering stage (adjust the orientation of the weight vector to minimize the orientation loss) and driving stage (fix the orientation and only scale the modulus of the vector to minimize the modulus loss).\
VecQ quantization is integrated in DNN training flow for weight and activation data, and the weight is treated as normally distributed to simplify the computing process. After evaluing it on image classification and comparing it with state-of-the-art solutions like BWN or TWN, it's concluded that it outperforms most of the solutions using the same ditwidth consiguration; and that it even provide a wider range of coverage.
<br/>
<br/>

To bridge the gap between fast DNN model designs and hardware accelerator, a first solution is to use DNNBuilder. It's an end-to-end automation framework that can transform DNN designs from deep learning frameworks to highly optimized hardware development on FPGAs. It produces the DNN accelerators by first designing and training them using deep learning frameworks that employ both CPU and GPU. The feedback function provides hardware metric estimations so that users can update the network design if judged not efficient enough.\
After that, the network definition files and trained parameters are used in the generation step, that uses the network layers to be mapped to the basic building blocks of the generated accelerator. The code is then generated for FPGA-based instances.\
Finally, the DNN accelerator is instancied in FPGA with an unified interface and concluded ready for eventual deployment.\
To deliver high throughput performance and promising real-time response, a fine-grained layer-based pipeline is used in place of a conventionnal pipeline; it reduces latency (9.92ms vs 411.99ms when handling the same object detection DNN model) while using the same number of layers. Moreover, a column-based cache is used to reduce on-chip memory utilization during DNN interference; it can reduce 43 times on-chip memory usage compared to another accelerator.\
The designs are demonstrated by accelerating popular AI workloads on an embedded platform, and the DNNBuilder reaches the best performance and efficiency, even without using batch processing.

As there are problems because of the fast-growing complexity of new applications for computing systems, High-Level Synthesis (HLS) can be a solution to spend less time compiling. It transform design input written in high-level languages to hardware descriptions in RTL; here in python with PyLog.\
It's a python-based programming flow for FPGAs that presents an unified programming model for host and acceleraor logic with consistent python syntax. Only the FPGA kernel function, decorated with `@pylog` needs to be compiled with PyLog; the rest of the code is interpreted by the standard python interpreter running on CPU, and will become the host program of the accelerator system.\
Some useful features of PyLog used in this paper are the high-level operators, that allow the user to express computation patterns at high level and enable a better compiler? They not only simplify the programming for user but also pass more information or computation to the compiler compared to c or c++ programming.\
The type interference and type checking is not implemented natively but is critical in PyLog since the same operator can have a totally different meaning if applied to a different shape. Finally, the compiler is optimized thanks to PyLog using its own intermediate representation for the input code that the code analysis and transformation pass through to perform a sequence of optimizations.\
PyLog is more user-friendly as it only need 30% of the code length of HLS C. Moreover, it provides better expressiveness and allow the users to describe the computation with fewer lines of code. And after using a real-world benchmark to evaluate the performance of the generated acceleraor, its said to achieve 3.17 times and 1.24 speedup over CPU baseline and manually optimized accelerators.
<br/>
<br/>

Finally, we know of three efficient optimization techniques.\
The first one is hardware-aware neutral architecture search (NAS), which is the automated process of neural architectural design; it produces many state-of-the-art network, and require search speace, search algorithm and network evaluation. Search space includes all possible network architectures that follow a defined template; search algorithm can greatly influence the efficiency of the search and the effectiveness of the final network architecture. It can be supernet or sampling based. Finally, network evaluation is the key for efficient NAS, but can be expensive due to network training so some approaches have been proposed to make the evaluation faster.

With the need of deploying power-hungry DNNs into resource constrained devices, hardware-aware NAS seems to be one of the most promising techniques but there is a great amount of work that adopt a specific hardware and require different hardware-cost metrics.

A second optimization technique is FPGA and DNN co-design. It was first proposed by Hao and Cheng in «Deep Neural Network Model and FPGA Accelerator Co-design: Opportunities and Challenges», and later on, a framework was implemented with a hardware-oriented bottom-up DNN model design (Auto-DNN) and a DNN-driven top-down FPGA accelerator design (Auto-HLS).\
The key to co-design are *Bundles*, as executing both Auto-DNN and Auto-HLS is made by proposing thoses basic building blocks that can be used to build both of them. So selecting an optimal bundle is non trivial given the large design space and long DNN training time; the search space must be narrowed as early as possible.\
The unfavorable bundles are filtered at early stage. Then, some analytical models for the overall DNN resource utilization and latency are made, with which the bundles are evaluated and selected by being replicated *n* times to build a DNN and train it. The bundles that are on the pareto curve (set of efficient solutions) are kept for the last filter.\
After selecting the top-n promising bundles candidates, DNN are created and updated for each one until the latency target is met. If we compare to the 1-st place winner of the FPGA category, there is a 6.2% higher IoU (Intersection over Union), 40% lower power use and 2.5 times better energy efficiency.


Finally, we can considerate EDD (Efficient Differentiable DNN), as it's a more generalized and unified approach; and is also a fully simultaneous, efficient differentiable DNN architecture and implementation co-search methodology.\
The key is to fuse the design space of the DNN architecture search and hardware implementation search. To carry out both the DNN architecture and hardware accelerator co-search, the loss function has to be minimized. Its differenciable has to be formulated with respect to the search spaces; moreover, by decending this loss function on the validation set, the search spaces can be learned simultaneously.\
The results of EDD optimizations are demonstrated on a subset of ImageNet dataset randomly sampled from 100 classes, and target 3 hardware architectures that each have a search DNN model (EDD-Net) where a single processing element is resued by all layers. Each model is produced through EDD within a 12-hours search on a P100 GPU.\
We can conclude that EDD-Net-1 reaches similar or better accuracy compared with the state-of-the-art DNN models and other NAS while achieving the shortest interference latency. EDD-Net-2 delivers the shortest latency on FPGA among all the DNNs and EDD-Net-3 achieves higher throughput with a much higher accuracy comparing with the state-of-the-art (40.2 fps vs 27.7 fps for 7.7% and 10.0% top-5 error).
<br/>
<br/>

To conclude, the high computation, memory demand and diverse application-specific requirements make developping DNN-based AI application challenging, and this paper aim to show that efficient machine learning algorithm, accelerator and compiler design, and various co-design and optimization strategies are good methods to overcome theses challenges.\
In the future, embedded AI solutions will involve more effective and comprehensive design methods, as their algorithms designs (ELB-NN and VecQ) can adopt more advanced quantization scheme to minimize network compression loss.\
The future works will considerate more diverse network architecture and layer-wise data distribution, as well as extend DNNBuilder and PyLog to create framework and tools for hardware design, synthesis, and workload compilation for a smoother accelerator design process.
