# Compilation and Optimizations for Efficient Machine Learning on Embedded Systems

This paper was published the 26th of August 2022, and written by Xiaofan Zhang, Yao Chen, Cong Hao, Sitao Huang, Yuhong Li, and Deming Chen. It focuses on the fact that the development of Deep Neural Networks (DNN) have been made accessible to the public, which makes machine learning more complicated, needing more resources to run, ...\
And as the evolutions of hardware are much slower than thoses on software, it has become much more complicated to run DNN on hardware today, especially for embedded and real-time services. This paper seeks to find ways to be more efficient for DNN on embedded systems.

Some methods for efficient DNN design on embedded systems already exist. As DNN algorithm have millions of parameters and billions of operations, they focus on network compression to reduce network complexities and the amount of data transmitted. They can also lower computer and memory demands, for example use quantized data to represent float parameters that would take much more space.\
To reduce the redundancy of DNN structures, the method of *network pruning* has been developped. As the major benefit of network compression come from reducing the fullt-connected (FC) layer, there are significant reductions can be achieved on classic DNN structures, lke AlexNet and VGG-16. Whereas more sophisticated algorithms are required for latter DNNs with fewer FC layers.\
Other methods can be building domain-specific hardware accelerators, or even on the software side with accelerations libraries on the CPU or kernel optimization on GPU. Theses customized accelerators can be implemented on FPGAs and ASICS, but there are some challenges, such as the hardware design process, verification problems, or even the time-consuming design space exploration during the DNN development.\
Finally, researchers proposed algorithms and accelerators co-design and co-search to solve the edge AI challenge. With that, software and hardware metrics are considered simultaneously. The neural architecture search (NAS) has been successfull for DNN models as the hardware-aware NAS aims to deliver high accuracy models with hardware efficiency.

Machine learning requires high inference accuracy, aggressive inference speed, throughput and energy efficiency to meet the real-life demand that is increasing more and more. For that, they rely on hardware-efficient DNN.\
The extremely low bit-width neural network (ELB-NN) is proposed to enhance energy efficiency when running image classification on embedded FPGA. It has a hybrid quantization scheme, which mean that different quantization schemes are involved for the network's parameters and activations, and that can go all the way to binary.\
After doing an analysis with AlexNet, it's concluded that its strong point is the robustness of the precision of parameters, but that the precision of activation impacts classification accuracy. However, it's possible to bring back the accuracy by increasing the model complexity. On the hardware side, the ELB-NN can achieve throughput performance up to 10.3 TOPS (how many computing operations an AI accelerator can handle in one second at 100% utilization) on the accelerator, which outperforms previous designs.\
Another ML algorithm is the Vectorized Quantization (VecQ), a training quantization based on a novel quantization loss measurement metric called *vector loss*. To quantisize with this vector loss, we use the square of the euclidean distance of the date before and after it to represent its loss, also called L2 distance. The orientation loss shows that the quantization takes 2 stages to minimize the 2 losses independently: the steering stage (adjust the orientation of the weight vector to minimize the orientation loss) and driving stage (fix the orientation and only scale the modulus of the vector to minimize the modulus loss).\
VecQ quantization is integrated in DNN training flow for weight and activation data, and the weight is treated as normally distributed to simplify the computing process. After evaluing it on image classification and comparing it with state-of-the-art solutions like BWN or TWN, it's concluded that it outperforms most of the solutions using the same ditwidth consiguration; and that it even provide a wider range of coverage.
